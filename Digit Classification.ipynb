{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Classification Using Neural Networks\n",
    "\n",
    "The MNIST dataset comprises 60,000 training examples and 10,000 test examples of the handwritten digits 0â€“9, formatted as 28x28-pixel monochrome images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "inputData=input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random as ran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainSize(number):\n",
    "    print('Total Images in Training Dataset= '+ str(inputData.train.images.shape))\n",
    "    xTrain= inputData.train.images[:number,:] # selecting 'number' number of rows from all the data we have as our training data\n",
    "    print('xTrain Examples Loaded=' + str(xTrain.shape))\n",
    "    yTrain= inputData.train.labels[:number,:]\n",
    "    print('yTrain Examples Loaded=' +str(yTrain.shape))\n",
    "    return xTrain, yTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testSize(number):\n",
    "    print('Total Images in Testing Dataset= '+ str(inputData.test.images.shape))\n",
    "    xTest= inputData.test.images[:number,:] # selecting 'number' number of rows from all the data we have as our test data\n",
    "    print('xTest Examples Loaded=' + str(xTest.shape))\n",
    "    yTest= inputData.test.labels[:number,:]\n",
    "    print('yTest Examples Loaded=' +str(yTest.shape))\n",
    "    return xTest, yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def displayDigit(number):\n",
    "    print(yTrain[number])\n",
    "    label= yTrain[number].argmax(axis=0) # interprets the one-hot encoded label\n",
    "    image= xTrain[number].reshape([28,28]) # interprets the image vector of size 1x784\n",
    "    plt.title('Example:%d Label:%d' %(number,label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap(\"gray_r\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images in Training Dataset= (55000, 784)\n",
      "xTrain Examples Loaded=(55000, 784)\n",
      "yTrain Examples Loaded=(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "xTrain, yTrain = trainSize(55000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xTrain Data\n",
    "\n",
    "<img src=\"IMAGES/1.png\">\n",
    "\n",
    "# yTrain Data\n",
    "\n",
    "<img src=\"IMAGES/2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20195977320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayDigit(ran.randint(0,xTrain.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"IMAGES/softmax-regression-scalargraph.png\" style=\"width:600px;height:304px;\"></center>\n",
    "\n",
    "Softmax Function: https://en.wikipedia.org/wiki/Softmax_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow builds the network architecture which you feed with inputs and run during **sessions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess= tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating placeholders that are the exact size of the input. Placeholders are used to feed the network with input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32, shape=[None,784]) #None here means any multiples of 1x784 vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ybar= tf.placeholder(tf.float32, shape=[None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W= tf.Variable(tf.zeros([784,10]))\n",
    "b= tf.Variable(tf.zeros([10])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y= tf.nn.softmax(tf.matmul(x,W)+ b) # Multinomial logistic Regression with softmax activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xTrain, yTrain = trainSize(5)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(sess.run(y,feed_dict={x: xTrain}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crossEntropy= tf.reduce_mean(-tf.reduce_sum(ybar*tf.log(y), reduction_indices=[1])) # reduce_sum Computes the sum of elements across dimensions of a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMAGES/Parabola-antipodera.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images in Training Dataset= (55000, 784)\n",
      "xTrain Examples Loaded=(5500, 784)\n",
      "yTrain Examples Loaded=(5500, 10)\n",
      "Total Images in Testing Dataset= (10000, 784)\n",
      "xTest Examples Loaded=(10000, 784)\n",
      "yTest Examples Loaded=(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "xTrain, yTrain = trainSize(5500)\n",
    "xTest, yTest = testSize(10000)\n",
    "learningRate = 0.1\n",
    "TrainSteps = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initialize= tf.global_variables_initializer()\n",
    "sess.run(initialize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training= tf.train.GradientDescentOptimizer(learningRate).minimize(crossEntropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correctPrediction= tf.equal(tf.argmax(y,1),tf.argmax(ybar,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy= tf.reduce_mean(tf.cast(correctPrediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:0  Accuracy =  0.5988  Loss = 2.1882\n",
      "Training Step:100  Accuracy =  0.8647  Loss = 0.580297\n",
      "Training Step:200  Accuracy =  0.879  Loss = 0.459822\n",
      "Training Step:300  Accuracy =  0.8866  Loss = 0.408572\n",
      "Training Step:400  Accuracy =  0.8904  Loss = 0.378081\n",
      "Training Step:500  Accuracy =  0.8943  Loss = 0.356976\n",
      "Training Step:600  Accuracy =  0.8974  Loss = 0.34105\n",
      "Training Step:700  Accuracy =  0.8984  Loss = 0.32835\n",
      "Training Step:800  Accuracy =  0.9  Loss = 0.317827\n",
      "Training Step:900  Accuracy =  0.9005  Loss = 0.308862\n",
      "Training Step:1000  Accuracy =  0.9009  Loss = 0.301065\n",
      "Training Step:1100  Accuracy =  0.9023  Loss = 0.29417\n",
      "Training Step:1200  Accuracy =  0.9029  Loss = 0.287995\n",
      "Training Step:1300  Accuracy =  0.9033  Loss = 0.282406\n",
      "Training Step:1400  Accuracy =  0.9039  Loss = 0.277303\n",
      "Training Step:1500  Accuracy =  0.9048  Loss = 0.27261\n",
      "Training Step:1600  Accuracy =  0.9057  Loss = 0.268267\n",
      "Training Step:1700  Accuracy =  0.9062  Loss = 0.264226\n",
      "Training Step:1800  Accuracy =  0.9061  Loss = 0.260449\n",
      "Training Step:1900  Accuracy =  0.9063  Loss = 0.256905\n",
      "Training Step:2000  Accuracy =  0.9066  Loss = 0.253566\n",
      "Training Step:2100  Accuracy =  0.9072  Loss = 0.250412\n",
      "Training Step:2200  Accuracy =  0.9073  Loss = 0.247422\n",
      "Training Step:2300  Accuracy =  0.9071  Loss = 0.244581\n",
      "Training Step:2400  Accuracy =  0.9066  Loss = 0.241876\n",
      "Training Step:2500  Accuracy =  0.9067  Loss = 0.239294\n",
      "Training Step:2600  Accuracy =  0.9068  Loss = 0.236825\n",
      "Training Step:2700  Accuracy =  0.9065  Loss = 0.234459\n",
      "Training Step:2800  Accuracy =  0.9061  Loss = 0.232189\n",
      "Training Step:2900  Accuracy =  0.9059  Loss = 0.230007\n",
      "Training Step:3000  Accuracy =  0.9059  Loss = 0.227906\n",
      "Training Step:3100  Accuracy =  0.9056  Loss = 0.225881\n",
      "Training Step:3200  Accuracy =  0.9055  Loss = 0.223927\n",
      "Training Step:3300  Accuracy =  0.9052  Loss = 0.222039\n",
      "Training Step:3400  Accuracy =  0.9052  Loss = 0.220213\n",
      "Training Step:3500  Accuracy =  0.9056  Loss = 0.218445\n",
      "Training Step:3600  Accuracy =  0.906  Loss = 0.216731\n",
      "Training Step:3700  Accuracy =  0.9059  Loss = 0.215068\n",
      "Training Step:3800  Accuracy =  0.906  Loss = 0.213454\n",
      "Training Step:3900  Accuracy =  0.9062  Loss = 0.211885\n",
      "Training Step:4000  Accuracy =  0.9061  Loss = 0.21036\n",
      "Training Step:4100  Accuracy =  0.906  Loss = 0.208875\n",
      "Training Step:4200  Accuracy =  0.9062  Loss = 0.207429\n",
      "Training Step:4300  Accuracy =  0.9061  Loss = 0.206019\n",
      "Training Step:4400  Accuracy =  0.9063  Loss = 0.204645\n",
      "Training Step:4500  Accuracy =  0.9064  Loss = 0.203304\n",
      "Training Step:4600  Accuracy =  0.9061  Loss = 0.201995\n",
      "Training Step:4700  Accuracy =  0.9061  Loss = 0.200716\n",
      "Training Step:4800  Accuracy =  0.9059  Loss = 0.199465\n",
      "Training Step:4900  Accuracy =  0.9057  Loss = 0.198243\n",
      "Training Step:5000  Accuracy =  0.9056  Loss = 0.197047\n"
     ]
    }
   ],
   "source": [
    "for i in range(TrainSteps+1):\n",
    "    sess.run(training, feed_dict={x: xTrain, ybar: yTrain})\n",
    "    if i%100 == 0:\n",
    "        print('Training Step:' + str(i) + '  Accuracy =  ' + str(sess.run(accuracy, feed_dict={x: xTest, ybar: yTest})) \n",
    "              + '  Loss = ' + str(sess.run(crossEntropy, {x: xTrain, ybar: yTrain})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
